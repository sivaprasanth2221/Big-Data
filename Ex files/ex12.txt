[21bad049@mepcolinux ex12]$cat classification.txt
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("WineClassification").getOrCreate()

# Load the wine dataset
df = spark.read.csv("/databricks-datasets/wine-quality/winequality-white.csv", header=True, inferSchema=True, sep=";")

# Assemble features into a single vector
assembler = VectorAssembler(inputCols=df.columns[:-1], outputCol="features")
dataset = assembler.transform(df)

# Split the data into training and test sets
train_data, test_data = dataset.randomSplit([0.7, 0.3], seed=1234)

# Train a LogisticRegression model
lr = LogisticRegression(labelCol="quality", featuresCol="features", maxIter=10)
lr_model = lr.fit(train_data)

# Make predictions
predictions = lr_model.transform(test_data)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="quality", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)
print(f"Test set accuracy = {accuracy}")

# Show the result
display(predictions)

OUTPUT:

Test set accuracy = 0.52990851513019

prediction
7
7
7
6
7
[21bad049@mepcolinux ex12]$cat clustering.txt
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.feature import VectorAssembler

# Assuming df is your Spark DataFrame containing the wine dataset
# and it has been loaded as shown in previous examples

# Assemble features into a single vector (excluding the label if present)
feature_columns = [col for col in df.columns if col != 'label']  # Adjust 'label' if your target column is named differently
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
dataset = assembler.transform(df)

# Trains a k-means model
kmeans = KMeans().setK(3).setSeed(1)  # SetK is the number of clusters to create
model = kmeans.fit(dataset)

# Make predictions
predictions = model.transform(dataset)

# Evaluate clustering by computing Silhouette score
evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

# Shows the result
centers = model.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)


OUTPUT:

Silhouette with squared euclidean distance = 0.5879289126242127
Cluster Centers:
[6.96918967e+00 2.93272484e-01 3.53615316e-01 9.34639359e+00
 5.17916296e-02 5.07373108e+01 1.96979964e+02 9.96297894e-01
 3.18301870e+00 5.14995548e-01 9.70716830e+00 5.55654497e+00]
[6.77864294e+00 2.73081201e-01 3.17525028e-01 4.16960512e+00
 4.08937709e-02 2.39363181e+01 9.60661846e+01 9.92331201e-01