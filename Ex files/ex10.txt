jonam@Jonam:~/kafka/ex10$ cat producer.py
from kafka import KafkaProducer

# Define Kafka configuration
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: str(v).encode('utf-8')  # Serialize messages to JSON
)

print("Enter features for prediction (comma-separated values). Type 'exit' to stop.")

while True:
    # Get user input
    user_input = input("Input features (e.g., 5.1,3.5,1.4,0.2): ")
    if user_input.lower() == "exit":
        break

    # Split input into a list of features

    producer.send('mlmodel',value=user_input)
    print(f"Sent: {user_input}")

# Ensure all messages are sent before exiting
producer.flush()
jonam@Jonam:~/kafka/ex10$ cat ml_stream.py
from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegressionModel
from pyspark.ml.clustering import KMeansModel
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import col, split

# Initialize Spark session
spark = SparkSession.builder.appName("KafkaSparkCSVStreaming").getOrCreate()

# Read streaming data from Kafka
df = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "mlmodel") \
    .load()



logistic_model = LogisticRegressionModel.load("logistic_regression_model")
clustering_model = KMeansModel.load("kmeans_model")

df = df.selectExpr("CAST(value AS STRING)")

# Split the CSV values into columns (assuming CSV structure: feature1, feature2, label)
df = df.withColumn("value", split(col("value"), ","))

df = df.select(
    col("value").getItem(0).cast("double").alias("feature1"),
    col("value").getItem(1).cast("double").alias("feature2"),
    col("value").getItem(2).cast("double").alias("feature3"),
    col("value").getItem(3).cast("double").alias("feature4")
)

# VectorAssembler for features
assembler = VectorAssembler(inputCols=["feature1", "feature2","feature3","feature4"], outputCol="features")
df = assembler.transform(df)

static_df = df.select("features")

logistic_predictions = logistic_model.transform(static_df)


clustering_predictions = clustering_model.transform(static_df)

results_df = logistic_predictions.select("features", "prediction").alias("logistic") \
    .join(clustering_predictions.select("features", "prediction").alias("cluster"),
          on="features", how="inner")

query = results_df \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
jonam@Jonam:~/kafka/ex10$