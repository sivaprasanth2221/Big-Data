exc 2

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

    public static class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            StringTokenizer tokenizer = new StringTokenizer(line);
            while (tokenizer.hasMoreTokens()) {
                word.set(tokenizer.nextToken());
                context.write(word, one);
            }
        }
    }

    public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");

        job.setJarByClass(WordCount.class);
        job.setMapperClass(WordCountMapper.class);
        job.setCombinerClass(WordCountReducer.class);
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}


hello 2
i 2
am 2
ram 1
sam 1



import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;

public class ElectricConsumption {

    
    public static class E_EMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

    private Text year = new Text();
    private IntWritable units = new IntWritable();

    
    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        StringTokenizer tokenizer = new StringTokenizer(line, ",");

        
        if (tokenizer.hasMoreTokens()) {
            String yearStr = tokenizer.nextToken().trim();
            year.set(yearStr);

            while (tokenizer.hasMoreTokens()) {
                try {
                    int unitsValue = Integer.parseInt(tokenizer.nextToken().trim());
                    units.set(unitsValue);
                    context.write(year, units);
                } catch (NumberFormatException e) {
                    
                    System.err.println("Skipping non-integer value: " + e.getMessage());
                }
            }
        } else {
            System.err.println("Skipping malformed line: " + line);
        }
    }
}


    // Reducer class
    public static class E_EReduce extends Reducer<Text, IntWritable, Text, IntWritable> {

        // Reduce function
        public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            int maxUnits = Integer.MIN_VALUE;
            int minUnits = Integer.MAX_VALUE;

            for (IntWritable value : values) {
                int currentUnits = value.get();
                if (currentUnits > maxUnits) {
                    maxUnits = currentUnits;
                }
                if (currentUnits < minUnits) {
                    minUnits = currentUnits;
                }
            }

            // Emit the year with max and min units
            context.write(new Text(key.toString() + " Max Usage"), new IntWritable(maxUnits));
            context.write(new Text(key.toString() + " Min Usage"), new IntWritable(minUnits));
        }
    }

    // Main function
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "max_min_electricityunits");

        job.setJarByClass(ElectricConsumption.class);
        job.setMapperClass(E_EMapper.class);
        job.setReducerClass(E_EReduce.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        job.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.TextInputFormat.class);
        job.setOutputFormatClass(org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.class);

        org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(job, new Path(args[0]));
        org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}


output

1979 Max Usage  43
1979 Min Usage  2
1980 Max Usage  31
1980 Min Usage  26
1981 Max Usage  36
1981 Min Usage  31
1984 Max Usage  43
1984 Min Usage  38
1985 Max Usage  45
1985 Min Usage  0




---------------------------------------------------------------

exc 3

[21badl04@mepcolinux b]$cat exc3.txt
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class UserPageCount {

    public static class TokenizerMapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text userId = new Text();
        private Text pageCountUrl = new Text();

        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString(), ",");
            while (itr.hasMoreTokens()) {
                String user = itr.nextToken();
                String pageCount = itr.nextToken();
                String url = itr.nextToken();
                userId.set(user);
                pageCountUrl.set(pageCount + "," + url);
                context.write(userId, pageCountUrl);
            }
        }
    }

    public static class IntSumReducer extends Reducer<Text, Text, Text, Text> {
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            List<String> pageCountUrlList = new ArrayList<>();
            for (Text val : values) {
                pageCountUrlList.add(val.toString());
            }

            // Sort based on the page count
            Collections.sort(pageCountUrlList, (a, b) -> {
                int pageCountA = Integer.parseInt(a.split(",")[0]);
                int pageCountB = Integer.parseInt(b.split(",")[0]);
                return Integer.compare(pageCountA, pageCountB);
            });

            StringBuilder sortedPageCounts = new StringBuilder();
            for (String pageCountUrl : pageCountUrlList) {
                sortedPageCounts.append(pageCountUrl).append(" ");
            }
            context.write(key, new Text(sortedPageCounts.toString().trim()));
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "user page count");
        job.setJarByClass(UserPageCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}




import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;

public class SearchUser {

    public static void main(String[] args) throws IOException {
        if (args.length != 2) {
            System.err.println("Usage: SearchUser <output-dir> <user-id>");
            System.exit(1);
        }

        String outputDir = args[0];
        String userId = args[1];

        try (BufferedReader br = new BufferedReader(new FileReader(outputDir + "/part-r-00000"))) {
            String line;
            boolean found = false;
            while ((line = br.readLine()) != null) {
                String[] parts = line.split("\t");
                if (parts.length == 2 && parts[0].trim().equals(userId)) {
                    System.out.println(parts[1]);  // Output the entire line after the user-id
                    found = true;
                    break;  // Assuming user-id is unique, exit loop once found
                }
            }
            if (!found) {
                System.out.println("No matching record found for user-id: " + userId);
            }
        }
    }
}



001     3,www.turorialspoint.com  4,www.javapoint.com
002     5,www.javapoint.com
003     2,www.analyticsvidhya.com


---------------------------------------------------------

exc 4

import java.io.IOException;
import java.util.HashSet;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

public class InvertedIndex {

    // Mapper class
    public static class InvertedIndexMapper extends Mapper<LongWritable, Text, Text, Text> {

        private Text name = new Text();
        private Text filename = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // Get the filename from the context
            String fileName = ((FileSplit) context.getInputSplit()).getPath().getName();
            filename.set(fileName);

            // Split the input line to extract the employee name
            String[] tokens = value.toString().split(",");
            if (tokens.length > 0) {
                name.set(tokens[0].trim());
                context.write(name, filename);
            }
        }
    }

    // Reducer class
    public static class InvertedIndexReducer extends Reducer<Text, Text, Text, Text> {

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            HashSet<String> fileSet = new HashSet<>();

            // Collect all unique filenames for each employee name
            for (Text value : values) {
                fileSet.add(value.toString());
            }

            // Join filenames into a single string
            StringBuilder fileList = new StringBuilder();
            for (String filename : fileSet) {
                if (fileList.length() > 0) {
                    fileList.append("\t");
                }
                fileList.append(filename);
            }

            context.write(key, new Text(fileList.toString()));
        }
    }

    // Driver class
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: InvertedIndex <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Inverted Index");

        job.setJarByClass(InvertedIndex.class);
        job.setMapperClass(InvertedIndexMapper.class);
        job.setReducerClass(InvertedIndexReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}


output

sam     sample1.csv     sample3.csv
dharun  sample3.csv
ram     sample1.csv     sample2.csv
jayaram sample2.csv
keshav  sample2.csv
naboth  sample1.csv
sagar   sample3.csv

------------------------------------------------------



import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class EmployeeInvertedIndex {

    // Mapper class
    public static class EmployeeMapper extends Mapper<LongWritable, Text, Text, Text> {

        private Text firstName = new Text();
        private Text fileName = new Text();

        @Override
        public void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {

            // Get the file name from the input split
            String file = ((org.apache.hadoop.mapreduce.lib.input.FileSplit) context.getInputSplit()).getPath().getName();

            // Each line in the file contains employee data: first name, last name, age, position
            String line = value.toString();
            String[] fields = line.split(",");

            if (fields.length > 1) {
                firstName.set(fields[0].trim()); // The first name is the first field
                fileName.set(file); // The file name

                // Emit the first name as the key and the file name as the value
                context.write(firstName, fileName);
            }
        }
    }

    // Reducer class
    public static class EmployeeReducer extends Reducer<Text, Text, Text, Text> {

        private Text result = new Text();

        @Override
        public void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {

            Set<String> files = new HashSet<>(); // To hold unique file names

            // Collect all unique file names in which the employee's first name appears
            for (Text val : values) {
                files.add(val.toString());
            }

            // Prepare the result: first name -> comma-separated file names
            StringBuilder sb = new StringBuilder();
            for (String file : files) {
                sb.append(file).append(" ");
            }

            result.set(sb.toString().trim());
            context.write(key, result); // Emit the first name and the list of file names
        }
    }

    // Main method to configure the MapReduce job
    public static void main(String[] args) throws Exception {

        if (args.length != 2) {
            System.err.println("Usage: EmployeeInvertedIndex <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Employee Inverted Index");

        job.setJarByClass(EmployeeInvertedIndex.class);
        job.setMapperClass(EmployeeMapper.class);
        job.setReducerClass(EmployeeReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        // Set input and output paths
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        // Wait for job completion
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}



---------------------------------


import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.*;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

public class TFIDF {

    // Mapper class
    public static class TFIDFMapper extends Mapper<LongWritable, Text, Text, IntWritable> {

        private Text word = new Text();
        private IntWritable one = new IntWritable(1);

        @Override
        public void map(LongWritable key, Text value, Context context)
                throws IOException, InterruptedException {

            // Tokenize the input line
            String line = value.toString().toLowerCase();
            StringTokenizer tokenizer = new StringTokenizer(line);

            // Get the document ID from the file name
            String fileName = ((FileSplit) context.getInputSplit()).getPath().getName();
            String docId = fileName.replaceAll(".txt", ""); // Assuming input files are named as doc1.txt, doc2.txt, etc.

            // Emit <word@docId, 1> for each token
            while (tokenizer.hasMoreTokens()) {
                String token = tokenizer.nextToken();
                word.set(token + "@" + docId);
                context.write(word, one);
            }
        }
    }

    // Reducer class
    public static class TFIDFReducer extends Reducer<Text, IntWritable, Text, DoubleWritable> {

        private DoubleWritable tfidf = new DoubleWritable();
        private int numDocuments = 3; // Assuming there are 3 documents in total

        @Override
        public void reduce(Text key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {

            // Calculate term frequency (TF)
            int termFrequency = 0;
            for (IntWritable val : values) {
                termFrequency += val.get();
            }

            // Extract document ID
            String docId = key.toString().split("@")[1];

            // Count how many documents contain the term
            int docCount = 1; // In this case, we'll assume it's 1 for simplicity (you can modify to count actual doc occurrences)

            // Calculate IDF (inverse document frequency)
            double idf = Math.log((double) numDocuments / docCount);

            // Calculate TF-IDF
            double tf = (double) termFrequency;
            double tfidfValue = tf * idf;

            tfidf.set(tfidfValue);

            // Emit <word@docId, tfidf>
            context.write(key, tfidf);
        }
    }

    // Main method
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: TFIDF <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "TF-IDF Calculation");
        job.setJarByClass(TFIDF.class);
        job.setMapperClass(TFIDFMapper.class);
        job.setReducerClass(TFIDFReducer.class);

        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}